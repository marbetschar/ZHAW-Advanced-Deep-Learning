{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccbf74e-1fcb-40c2-98d7-c30d0f73f252",
   "metadata": {},
   "source": [
    "# Deep Q Learning\n",
    "\n",
    "In this exercise we will build a simple deep Q-learning agent from scratch. For this we need to look at the following tasks:\n",
    "* How do we specify the model\n",
    "* How do we calculate an action\n",
    "* How do we sample episodes\n",
    "* How do we train the model\n",
    "\n",
    "We will develop this by implementing a class for the agent. We will use a package called jdc, that will allow us to split the implementation of a class over several cells.\n",
    "\n",
    "We will be using torch for the implementation of the neural network. There is also a version of this exercise using keras, you can solve either one.\n",
    "\n",
    "In this version of the exercise, you have to implement more :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18836b84-bfcc-4d2e-a25c-cd1d9dbef9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jdc\n",
    "!pip install ipywidgets\n",
    "!pip install pyglet\n",
    "!pip install pygame\n",
    "import jdc\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyglet\n",
    "import ipywidgets\n",
    "from IPython import display\n",
    "\n",
    "print(f'Matplotlib version: {matplotlib.__version__}')\n",
    "print(f'Pyglet version: {pyglet.__version__}')\n",
    "print(f'Ipywidgets version: {ipywidgets.__version__}')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d78c58-adcf-4645-9916-1a3316121f12",
   "metadata": {},
   "source": [
    "## Example: Cart Pole\n",
    "We will use an environment from OpenAI for this exercise. The goal is to balance a pole by moving the attached cart to the left or to the right. As the pole should be balanced as long as possible, the reward for each time step is +1. An episode is done when the angle of the pole becomes too large.\n",
    "\n",
    "The observation space gives some measurements about the pole, for example the angle. However, we actually do not need to know the specific details, as the neural network will just learn using the input.\n",
    "\n",
    "The actions are to move the cart to the left or to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fccfa-3da9-4939-a71e-f1ae82ceb64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "\n",
    "print(f'Observation space: {env.observation_space}')\n",
    "print(f'Action space: {env.action_space}')\n",
    "print(f'Sample from the observation space: {env.observation_space.sample()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b6bac-94a9-42ec-ad59-9a5bade325e3",
   "metadata": {},
   "source": [
    "The environment has a render function that we can use to display the state. The parameter 'render_mode' is used to specify the mode. For standalone applications, we can use 'human' that will open a window. Here we get the image as an array and display it using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20961d56-4fe5-45d4-b67b-b791cb864706",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc811ef-a89b-4dd0-b385-c199d8a535d5",
   "metadata": {},
   "source": [
    "We can also try to render a sequence of images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d4e76-70ea-45b4-9b90-205cf6dbeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_environment(env):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off') \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "\n",
    "# Test the function with a few steps\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.step(env.action_space.sample())  # Take a random action\n",
    "    display_environment(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72df12-f213-4217-84fe-4b18b9bbdb94",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "We will define a class for the neural network (the model) that will be used for the agent. We will pass the number of observation values and the number of actions values as parameters.\n",
    "\n",
    "The model must calculate the q function for each state. We will also need to access those values for evaluation.\n",
    "\n",
    "Build a sequential model in torch using at least two dense layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9c401-f28d-4ed5-bfde-5990b8f3de02",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f57f2a0435bd064",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, n_obs, n_action):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        # generate a sequential model in a internal variable (for example self.fc)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward should just call your model\n",
    "\n",
    "\n",
    "    def q_values(self, obs):\n",
    "        obs = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q = self.forward(obs)\n",
    "        return q.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787f0e3-966e-4d85-94f8-e3950d45f90a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-02f08792391930df",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "m = DQNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "print(m)\n",
    "\n",
    "\n",
    "obs_sample = env.observation_space.sample()\n",
    "print(obs_sample)\n",
    "\n",
    "# models xpect a batch of data, so we have to add a dimension\n",
    "obs_batch = np.expand_dims(obs_sample, axis=0)\n",
    "action_values = m.q_values(obs_batch)\n",
    "assert action_values.shape == (1,2)\n",
    "\n",
    "print(action_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cd294-1491-4191-a25b-de8aafcd7762",
   "metadata": {},
   "source": [
    "## Agent class\n",
    "\n",
    "Now we are ready to implement the agent class. Check the parameters and the descriptions as they will be used in the implementation.\n",
    "\n",
    "The only step missing in the initialisation below is the optimizer and the loss function. There are different optimizers available, either standand SGD, Adam or RMSprob would be possible and should be initialized with the learning rate given in the parameters.\n",
    "\n",
    "What is the loss function that we have to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc0213-5dc9-4f23-ab2d-1d44d983a144",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c19c66d131c3fbd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, observation_space, action_space,\n",
    "             gamma: float,\n",
    "             epsilon: float, epsilon_decay: float, epsilon_min: float,\n",
    "             learning_rate: float, training_frequency: int, target_update_frequency: int,\n",
    "             tau : float, use_double_dqn: bool,\n",
    "             batch_size: int, memory_size: int):\n",
    "        \"\"\"\n",
    "        Initialise the agent.\n",
    "        Args:\n",
    "            observation_space: The observation space of the environment\n",
    "            action_space: The action space of the environment\n",
    "            gamma: The discount factor\n",
    "            epsilon: The initial epsilon value for the epsilon-greedy policy\n",
    "            epsilon_decay: The decay factor for the epsilon value\n",
    "            epsilon_min: The minimal epsilon value after which it will not be decayed further\n",
    "            learning_rate: The learning rate for the optimizer\n",
    "            training_frequency: The frequency (in steps) of training the model\n",
    "            target_update_frequency: The frequency (in steps) of updating the target model\n",
    "            tau: weight of the new model in the target update \n",
    "            use_double_dqn: use double q learning\n",
    "            batch_size: The batch size for training (sampled from the memory)\n",
    "            memory_size: The size of the memory for storing experiences\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "    \n",
    "        # hyperparameters from parameters\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.training_frequency = training_frequency\n",
    "        self.target_update_frequency = target_update_frequency\n",
    "    \n",
    "        self.use_double_dqn = use_double_dqn\n",
    "        self.tau = tau\n",
    "\n",
    "        self.nr_training_steps = 0\n",
    "    \n",
    "        # internal variables\n",
    "        self.nr_steps = 0\n",
    "    \n",
    "        self.last_action = None\n",
    "        self.last_obs = None\n",
    "    \n",
    "        # build the models\n",
    "        self.model = DQNetwork(observation_space.shape[0], action_space.n)\n",
    "        self.target_model = DQNetwork(observation_space.shape[0], action_space.n)\n",
    "\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.loss = None \n",
    "\n",
    "        # generate the optimizer and loss function in the variables below\n",
    "\n",
    "        self.optimizer = \n",
    "        self.loss = \n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the agent to the initial state\n",
    "        \"\"\"\n",
    "        self.nr_steps = 0\n",
    "        self.last_action = None\n",
    "        self.last_obs = None\n",
    "        self.nr_training_steps = 0\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Update the target model with the weights from the current model.\n",
    "        \"\"\"\n",
    "        target_net_state_dict = self.target_model.state_dict()\n",
    "        policy_net_state_dict = self.model.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * self.tau + target_net_state_dict[key] * (1 - self.tau)\n",
    "        self.target_model.load_state_dict(target_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf988c-24c5-4b8e-ad91-e79fec78d7b4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6638146a1a48950b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "q_agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.9,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9999,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=2,\n",
    "                       tau=0.005,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=256,\n",
    "                       memory_size=10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b09c6-c2b2-45dc-81e2-c87179dab829",
   "metadata": {},
   "source": [
    "## Calculating actions\n",
    "\n",
    "Calculating actions is done with an epsilon greedy policy. However, for evaluation it is often suitable to use the greedy policy instead. So we add a parameter `stochastic`, if it is True then the epsilon-greedy policy is used, if not the greedy policy is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c4b3c-82ec-456a-beb4-f94a3bc1e7b3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-941a10c9caf2e5a3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "def calculate_action(self, obs, stochastic: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the action for the given observation.\n",
    "    Args:\n",
    "        obs: the observation\n",
    "        stochastic: whether to use a stochastic (epsilon greedy) policy or not\n",
    "    Returns:\n",
    "        the action\n",
    "    \"\"\"\n",
    "    if not stochastic or np.random.rand() > self.epsilon:\n",
    "        # calculate greedy action\n",
    "        with torch.no_grad():\n",
    "            action_value = self.model.q_values(obs)\n",
    "        return np.argmax(action_value)\n",
    "    else:\n",
    "        # calculate random action\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490aa0b-73f6-4001-8ee5-0387ff91a8dc",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b52d56ccd08d24e1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.9,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9999,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=2,\n",
    "                       tau=0.005,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=256,\n",
    "                       memory_size=10000)\n",
    "obs = env.observation_space.sample()\n",
    "a = agent.calculate_action(obs)\n",
    "assert env.action_space.contains(a)\n",
    "a = agent.calculate_action(obs, stochastic=False)\n",
    "assert env.action_space.contains(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374cbec0-6b65-40b4-9bbf-c0c136afd366",
   "metadata": {},
   "source": [
    "## Add the steps\n",
    "\n",
    "Next we will add two step function as in the previous implementations of an agent last week. If the reward is equal to None, then this is the first step in the environment.\n",
    "\n",
    "The `step` method is called for all  steps. In the step method we need to\n",
    "* Save the current experience (S, A, R, S', done) in the memory.\n",
    "* Calculate the next action\n",
    "* Save action and observation for next step\n",
    "* train the model every couple of steps\n",
    "* update the target model every couple of steps\n",
    "\n",
    "You have to fill in the code for the first three items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922fa8c8-bc5d-4552-8709-91c6a8c9c421",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f11d5c8957197292",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def step(self, obs, reward: float, done: bool):\n",
    "\n",
    "    if reward == None:\n",
    "        self.last_obs = obs\n",
    "        self.last_action = self.calculate_action(obs)\n",
    "        return self.last_action        \n",
    "\n",
    "    # now we have a new state and reward from the last action and can add that to the buffer (memory)\n",
    "\n",
    "    # so the buffer is (S, A, R, S', done)\n",
    "    self.memory.append((self.last_obs, self.last_action, reward, obs, done))\n",
    "\n",
    "    # calculate the next action\n",
    "    self.last_action = self.calculate_action(obs)\n",
    "    self.last_obs = obs\n",
    "\n",
    "    self.nr_steps += 1\n",
    "\n",
    "    if self.nr_steps % self.training_frequency == 0:\n",
    "        self.train_model()\n",
    "\n",
    "    if self.nr_steps % self.target_update_frequency == 0:\n",
    "        self.update_target_model()\n",
    "\n",
    "    return self.last_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e1f9b-2b90-4d3f-94b7-adf575b31e33",
   "metadata": {},
   "source": [
    "## Train the model function\n",
    "\n",
    "The last thing to do is now to train the model. For this we have to \n",
    "* Sample from the memory to get a batch of observations, actions, rewards, next observations and dones\n",
    "* Calculate a better estimate for the q values of the current observation using q-learning\n",
    "* Fit the model to the updated values using gradient descend on the loss\n",
    "* Decay the epsilon value\n",
    "\n",
    "In torch, the gradient descend step has to be calculated in the code. Todo this:\n",
    "* Calculate the model output\n",
    "* Calculate the loss function\n",
    "* Clear the gradient (using self.optimizer.zero_grad())\n",
    "* Calculate a backwards step (resulting in the gradient)\n",
    "* Apply the step in the optimizer\n",
    "\n",
    "Note that between calculating the model, and calculating the loss function, you will have to calculate the target function (using the target_model), as no gradients are required on the target model, this code should be within a `with torch.no_grad()` block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5506a-0c18-4433-8b1d-529177bc2cf4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe51b24157ad1aa1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train_model(self):\n",
    "    # not enough samples yet\n",
    "    if len(self.memory) < self.batch_size:\n",
    "        return\n",
    "\n",
    "    q_value_target = np.zeros((self.batch_size))\n",
    "    # get the indices of the batch\n",
    "    indices = np.random.choice(len(self.memory), self.batch_size, replace=False)\n",
    "\n",
    "    # we need to sample this by list comprehension, as the memory is a list of tuples and not a numpy array\n",
    "    obs = np.array([self.memory[i][0] for i in indices])\n",
    "    actions = np.array([self.memory[i][1] for i in indices])\n",
    "    rewards = np.array([self.memory[i][2] for i in indices])\n",
    "    obs_next = np.array([self.memory[i][3] for i in indices])\n",
    "    dones = np.array([self.memory[i][4] for i in indices])\n",
    "\n",
    "    # we calculate the current q value that the model predicts and extract the value for the action taken\n",
    "    # size (batch x actions\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "    # add one dimension to actions, as we need to select the q value for each action using gather\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.int64)\n",
    "    actions_tensor.unsqueeze_(1)\n",
    "    q_value_current = self.model(obs_tensor)\n",
    "    q_value_current = q_value_current.gather(dim=1, index=actions_tensor)\n",
    "  \n",
    "    # we now calculate a better estimate for this value using the reward and the q value of the next state\n",
    "    # (this is calculated by the target model, that is not trained but updated every few steps)\n",
    "    obs_next_tensor = torch.tensor(obs_next, dtype=torch.float32)\n",
    "\n",
    "    dones_tensor = torch.tensor(dones, dtype=torch.float32)\n",
    "    ones_tensor = torch.ones_like(dones_tensor)\n",
    "    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
    "\n",
    "    # TODO: \n",
    "    # - calculate the update to the q_value\n",
    "    # - calculate the loss function\n",
    "    # - calculate the gradient\n",
    "    # - apply the optimizer \n",
    "\n",
    "\n",
    "    self.nr_training_steps += 1\n",
    "    \n",
    "    # reduce epsilon\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e246cf-b487-4e93-acc9-1b18e3d1417e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-38fb10ebd1e39242",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test with a small version of the agent\n",
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.9,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9999,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=2,\n",
    "                       tau=0.005,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=4,\n",
    "                       memory_size=16)\n",
    "obs, info = env.reset()\n",
    "action = agent.step(obs, None, False)\n",
    "for i in range(20):\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    action = agent.step(obs, reward, done)\n",
    "\n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "        action = agent.step(obs, None, False)\n",
    "\n",
    "print(agent.nr_training_steps)\n",
    "assert agent.nr_training_steps > 4\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b96b12-e8dd-41f6-a8dc-6acd8c2f8223",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented a full DQN Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ef19c-2e2a-4d97-a173-34322b805f55",
   "metadata": {},
   "source": [
    "## Complete agent for training and evaluation.\n",
    "\n",
    "We will add some additional methods to the agent in order to train and evaluate it in an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b25676-7364-48ef-9ebb-9ac55e730bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train(self, env: gym.Env, nr_episodes_to_train: int,  eval_env: gym.Env, eval_frequency: int):\n",
    "    \"\"\"\n",
    "    Train the agent on the given environment for the given number of steps.\n",
    "    Args:\n",
    "        env: The environment on which to train the agent\n",
    "        nr_episodes_to_train: the number of episodes to train\n",
    "        eval_env: Environment for evaluation\n",
    "        eval_frequency: Frequency of evaluation of the trained agent\n",
    "    \"\"\"\n",
    "    nr_episodes = 0\n",
    "    while True:\n",
    "        obs, _ = env.reset()\n",
    "        a = self.step(obs, None, False)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            done = done or truncated\n",
    "            a = self.step(obs, reward, done)\n",
    "\n",
    "        nr_episodes += 1\n",
    "        if nr_episodes % eval_frequency == 0:\n",
    "            rewards = self.evaluate(eval_env, 10)\n",
    "            print(f'Evaluation: episode {nr_episodes}, epsilon: {self.epsilon} mean reward: {np.mean(rewards)}')\n",
    "\n",
    "        if nr_episodes >= nr_episodes_to_train:\n",
    "            return\n",
    "\n",
    "def evaluate(self, env: gym.Env, nr_episodes: int):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: the environment on which to evaluate the agent\n",
    "        nr_episodes: the number of episodes to evaluate\n",
    "    Returns:\n",
    "        the rewards for the episodes\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for e in range(nr_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        a = self.calculate_action(obs, stochastic=False)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        # some environments do not support truncated episodes, so we additionally check for a maximal number of steps\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a = self.calculate_action(obs, stochastic=False)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e050db9-1f7d-4290-9e6d-03c4a936fa69",
   "metadata": {},
   "source": [
    "## Example Training\n",
    "\n",
    "Here is an example with some hyperparameters and a short training time (that will not be enough to actually train the full agent). You can adjust the parameters and see if you get good results. But for handing in the exercise, put it back to a short training :-). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732052e5-bc4f-40d4-a496-18babc377d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = gym.make(environment_name)\n",
    "env_eval = gym.make(environment_name, render_mode='rgb_array')\n",
    "\n",
    " # Hyperparameters will be quite important here.\n",
    "q_agent = DQNAgent(env_train.observation_space, env_train.action_space,\n",
    "                   gamma=0.99,\n",
    "                   epsilon=0.9,\n",
    "                   epsilon_min=0.03,\n",
    "                   epsilon_decay=0.9999,\n",
    "                   learning_rate=0.0005,\n",
    "                   training_frequency=1,\n",
    "                   target_update_frequency=2,\n",
    "                   tau=0.005,\n",
    "                   use_double_dqn=True,\n",
    "                   batch_size=256,\n",
    "                   memory_size=10000)\n",
    "\n",
    "\n",
    "q_agent.train(env, nr_episodes_to_train=200, eval_env=env_eval, eval_frequency=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b3c3b-1d54-4e5a-8a07-cef082a51899",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.evaluate(env_eval, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7ac82-ea78-4f37-bb3d-86cc843a1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with a few steps\n",
    "obs, _ = env.reset()\n",
    "for _ in range(200):\n",
    "    action = q_agent.calculate_action(obs, stochastic=False)\n",
    "    obs, _, done, _, _ = env.step(action)  # Take a random action\n",
    "    display_environment(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c50de3-bccf-42ad-9a05-55694dffc277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
