{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4291daa-e8ea-4475-a1bf-96833fa13a82",
   "metadata": {},
   "source": [
    "# RNNs and sequence processing\n",
    "\n",
    "## Tutorial and homework 1: Sentiment Analysis\n",
    "\n",
    "We will look at a simple example of sequence processing in sentiment analysis. Sentiment analysis is used to determine the emotion in a text. \n",
    "\n",
    "We will use a data set from IMDB consisting of movie reviews and are only interested if they are positive or negative (http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "In addition to the libraries installed last time, we will also neet torchtext and datasets: \n",
    "\n",
    "pip install torchtext\n",
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "ffb3bca6-8711-4bd1-a54c-eae0a4966c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T12:53:56.909538Z",
     "start_time": "2025-09-25T12:53:56.850620Z"
    }
   },
   "source": [
    "import collections\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import tqdm\n",
    "import torchtext.data\n",
    "import torchtext.vocab"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcollections\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdatasets\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'datasets'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9cb30-946b-45b8-9531-9984249cfa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be version 0.18.0\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9077d-ddea-4a6c-9a65-4f45814e2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5969ca5-546f-4b87-b0fe-59caa38f2e80",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing the IMDb Dataset\n",
    "\n",
    "Weâ€™ll use datasets to load and preprocess the IMDb dataset (it is also available from torchtext)\n",
    "- TorchText makes it easy to handle text data, including tokenization, padding, and batch generation.\n",
    "- IMDB Dataset consists of movie reviews labeled as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8a673-db3f-4a5b-bb24-35d98aafe6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344b68f-b677-494c-9cae-bf066f0d401a",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "We will use a tokinizer to split the sentences in words. This tokenizer will normalize english text first and then extract the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645d12b-a223-433f-94be-5dfb6f6db617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a90d26-54cc-4020-ac4c-7a454d84723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, tokenizer, max_length):\n",
    "    tokens = tokenizer(example[\"text\"])[:max_length]\n",
    "    length = len(tokens)\n",
    "    return {\"tokens\": tokens, \"length\": length}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffea7b-20d1-41e4-a3f9-e0e2283d6079",
   "metadata": {},
   "source": [
    "### Token sequence\n",
    "This will transfer the input data into sequences of words of a maximal length. While the model does not require the sequences to have the same length, it is needed for batch processing later.\n",
    "\n",
    "We then split the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ddde67-d79b-4dab-b7e1-b0b1d07b5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "train_data = train_data.map(\n",
    "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "test_data = test_data.map(\n",
    "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244709b-2255-409a-ad96-812d7dcf6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.25\n",
    "\n",
    "train_valid_data = train_data.train_test_split(test_size=test_size)\n",
    "train_data = train_valid_data[\"train\"]\n",
    "valid_data = train_valid_data[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c86b2-84da-485f-8aea-4320a151ce36",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Next we build the vocabulary of all words in the set and add two special \"words\" for unknown and padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ac450-e307-4c44-91c1-1355a12c91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "unk_index = vocab[\"<unk>\"]\n",
    "pad_index = vocab[\"<pad>\"]\n",
    "\n",
    "vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89ad0c-521d-406c-b657-2b457795b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, vocab):\n",
    "    ids = vocab.lookup_indices(example[\"tokens\"])\n",
    "    return {\"ids\": ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac783c13-40ef-49e5-873d-2030eceb5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323344c-4c4b-4970-b503-85bbec3bcfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.with_format(type=\"torch\", columns=[\"ids\", \"label\", \"length\"])\n",
    "valid_data = valid_data.with_format(type=\"torch\", columns=[\"ids\", \"label\", \"length\"])\n",
    "test_data = test_data.with_format(type=\"torch\", columns=[\"ids\", \"label\", \"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc2446-cff7-4f44-bc23-08570a39ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512352f1-7b44-4d98-92b3-4d3bc1f3a89c",
   "metadata": {},
   "source": [
    "### Creating Batches\n",
    "\n",
    "Next we create batches out of the data with the same length and store the indices, actual length and label in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56911a3d-9224-45e4-8885-b802954e274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_ids = [i[\"ids\"] for i in batch]\n",
    "        batch_ids = nn.utils.rnn.pad_sequence(\n",
    "            batch_ids, padding_value=pad_index, batch_first=True\n",
    "        )\n",
    "        batch_length = [i[\"length\"] for i in batch]\n",
    "        batch_length = torch.stack(batch_length)\n",
    "        batch_label = [i[\"label\"] for i in batch]\n",
    "        batch_label = torch.stack(batch_label)\n",
    "        batch = {\"ids\": batch_ids, \"length\": batch_length, \"label\": batch_label}\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc932aea-0a2d-4a22-a655-59bbe048027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1293e9a-2682-4cd8-9bf2-9bebcfa9edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3247b2-ec26-43ee-9506-83676edff0ed",
   "metadata": {},
   "source": [
    "### RNN Model\n",
    "\n",
    "Now we can build a model, we will use a RNN Model first. It includes\n",
    "\n",
    "- Embedding Layer: Turns words into dense vectors.\n",
    "- RNN Layer: A simple recurrent layer to capture sequence information.\n",
    "- Fully Connected Layer: To classify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9c695-3f42-474d-952b-8e56cfa1d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                          bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, ids, length):\n",
    "        # ids = [batch size, seq len]\n",
    "        # length = [batch size]\n",
    "        embedded = self.dropout(self.embedding(ids))\n",
    "        # embedded = [batch size, seq len, embedding dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, length, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "\n",
    "        output, output_length = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        # output = [batch size, seq len, hidden dim * n directions]\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n",
    "            # hidden = [batch size, hidden dim * 2]\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1])\n",
    "            # hidden = [batch size, hidden dim]\n",
    "        prediction = self.fc(hidden)\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f6041-9a3a-45b2-9f74-096205180508",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "output_dim = len(train_data.unique(\"label\"))\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = RNN(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    hidden_dim,\n",
    "    output_dim,\n",
    "    n_layers,\n",
    "    bidirectional,\n",
    "    dropout_rate,\n",
    "    pad_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815bd833-5744-4562-8300-a528502b2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b443ae8-14b2-4d83-b1ed-6f24c0eb41aa",
   "metadata": {},
   "source": [
    "### Pretrained embeddings\n",
    "\n",
    "The model will learn the embeddings too, which is a bit difficult from the limited data. Results will be much better if a pretrained model is used for that.\n",
    "\n",
    "We will use glove (https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "The following code will download and cache the vectors. The data set is quite large so you can also omit this first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7bb71-a00e-48cf-8d5e-3c317c372d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.GloVe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464211c9-4db4-4e77-8455-70f83212b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n",
    "model.embedding.weight.data = pretrained_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f737cf-b571-4e17-a6db-e1807380c98c",
   "metadata": {},
   "source": [
    "### Train function\n",
    "Next we need the functions to train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e87cdf-4ee3-4afd-82aa-7f1f362c68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    for batch in tqdm.tqdm(dataloader, desc=\"training...\"):\n",
    "        ids = batch[\"ids\"].to(device)\n",
    "        length = batch[\"length\"]\n",
    "        label = batch[\"label\"].to(device)\n",
    "        prediction = model(ids, length)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, desc=\"evaluating...\"):\n",
    "            ids = batch[\"ids\"].to(device)\n",
    "            length = batch[\"length\"]\n",
    "            label = batch[\"label\"].to(device)\n",
    "            prediction = model(ids, length)\n",
    "            loss = criterion(prediction, label)\n",
    "            accuracy = get_accuracy(prediction, label)\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accs.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "\n",
    "# calculate the accuracy, we could also use the metrics classes as in the last exercise\n",
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4656ed-a0c0-4ee2-92fa-05149acff513",
   "metadata": {},
   "source": [
    "Construct model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b516650-0708-40ca-a084-28698ab1938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c115c9-fd62-4a1d-99e4-dd183c7c43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        # test if it worked\n",
    "        x = torch.ones(1, device=device)\n",
    "        print('Using CUDA device')\n",
    "\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        x = torch.ones(1, device=device)\n",
    "        print('Using MPS device')\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eedbe0-9db9-424c-97e6-11ce731f3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e8747-2c7e-4a0c-a01c-63d5d0e3ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "metrics = collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train(\n",
    "        train_data_loader, model, criterion, optimizer, device\n",
    "    )\n",
    "    valid_loss, valid_acc = evaluate(valid_data_loader, model, criterion, device)\n",
    "    metrics[\"train_losses\"].append(train_loss)\n",
    "    metrics[\"train_accs\"].append(train_acc)\n",
    "    metrics[\"valid_losses\"].append(valid_loss)\n",
    "    metrics[\"valid_accs\"].append(valid_acc)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"lstm.pt\")\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    print(f\"train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}\")\n",
    "    print(f\"valid_loss: {valid_loss:.3f}, valid_acc: {valid_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c406868-19ba-4f98-b63e-8da9e4615d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73619b-bd02-45e8-a118-5e24949f30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(metrics[\"train_losses\"], label=\"train loss\")\n",
    "ax.plot(metrics[\"valid_losses\"], label=\"valid loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xticks(range(n_epochs))\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d253c2-2468-44c3-a19a-b2f16218302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(metrics[\"train_accs\"], label=\"train accuracy\")\n",
    "ax.plot(metrics[\"valid_accs\"], label=\"valid accuracy\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xticks(range(n_epochs))\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef296c-4c6c-4476-b588-061fd955bc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ff282-e43d-4b8a-aadb-70f68889bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, vocab, device):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = vocab.lookup_indices(tokens)\n",
    "    length = torch.LongTensor([len(ids)])\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
    "    prediction = model(tensor, length).squeeze(dim=0)\n",
    "    probability = torch.softmax(prediction, dim=-1)\n",
    "    predicted_class = prediction.argmax(dim=-1).item()\n",
    "    predicted_probability = probability[predicted_class].item()\n",
    "    return predicted_class, predicted_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06318769-b141-4687-8eea-295a94748f8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21eac8-0af8-4f7a-92b1-a79527efd3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, vocab, device):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = vocab.lookup_indices(tokens)\n",
    "    length = torch.LongTensor([len(ids)])\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
    "    prediction = model(tensor, length).squeeze(dim=0)\n",
    "    probability = torch.softmax(prediction, dim=-1)\n",
    "    predicted_class = prediction.argmax(dim=-1).item()\n",
    "    predicted_probability = probability[predicted_class].item()\n",
    "    return predicted_class, predicted_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8fc23a-b96e-4073-bbea-9198e5ec3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is terrible!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43b194-c04a-41c8-8f15-5cfd4669a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is great!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b1b32b-1368-48c9-9a3b-24fb4dca7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is not terrible, it's great!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4b1e7-ca27-4931-b2e5-430c2b473ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is not great, it's terrible!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca8f99-cc9a-4cdc-94ef-1cd5e0c01568",
   "metadata": {},
   "source": [
    "## Exercise 1: LSTM\n",
    "\n",
    "The model should not be bad yet (specially if pretrained embeddings are used). Can you replace the RNN with a LSTM and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeed48d-a061-4475-a01f-eb0503f191f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f39a319-9f6d-4097-9ac9-6cbc253ae73e",
   "metadata": {},
   "source": [
    "## Exercise 2: Attention and Transformer (hard)\n",
    "\n",
    "Even as this is not a seq2seq tasks, it would be possible to use attention and transformer ideas here. For example the encoder part and then just use a more simple decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732dca43-15f0-4962-9d1d-d7738349505e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
